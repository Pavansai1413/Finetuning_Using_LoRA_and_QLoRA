{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKNxaX66Rn-L",
        "outputId": "d486b011-2a44-435a-d04d-8b48173758da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=4324fed7874291726d7a4d177540fb271a78e3ef036c3e96e91a2c04a139c218\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfxVfIiHRtnj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the finetuned model parameters:"
      ],
      "metadata": {
        "id": "WdIVaXA1cr6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsxkNUTRRx3h"
      },
      "outputs": [],
      "source": [
        "hf_repo_id = \"PavansaiGundaram/fine_tuned_qwen_medical_qa_updated_version\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDWaGo-KR_a1"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_repo_id, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the model:"
      ],
      "metadata": {
        "id": "IU4Hvd5Mcyt2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0D9eRAmSOyd",
        "outputId": "ea7ce073-a302-4f4c-e724-26358772147d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 896)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the fine-tuned LoRA model\n",
        "peft_config = PeftConfig.from_pretrained(hf_repo_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    peft_config.base_model_name_or_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, hf_repo_id, is_trainable=False)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the test dataset:"
      ],
      "metadata": {
        "id": "TWBx_XTuc7ml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-SgF9l5gIN8"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset\n",
        "ds = load_dataset(\"eswardivi/medical_qa\")\n",
        "ds = ds.remove_columns([\"input\"]).rename_column(\"instruction\", \"input\")\n",
        "train_test_split = ds[\"train\"].train_test_split(test_size=307, train_size=6000, shuffle=True, seed=42)\n",
        "test_dataset = train_test_split[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGLk0-4sSjCW"
      },
      "outputs": [],
      "source": [
        "# Function to format the prompt for evaluation\n",
        "def prompt_instruction_format(sample):\n",
        "    return f\"\"\"### Instruction:\n",
        "Provide a concise and accurate medical answer in one sentence based on the input below. If the information is unknown, respond with 'I don’t know.'\n",
        "\n",
        "### Input:\n",
        "{sample['input']}\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the function for generating the response:"
      ],
      "metadata": {
        "id": "ZbAfMiD8dBFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_QlTSS2SoHK"
      },
      "outputs": [],
      "source": [
        "# Function to generate model response\n",
        "def generate_response(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the response part after \"### Response:\"\n",
        "    response = response.split(\"### Response:\")[-1].strip() if \"### Response:\" in response else response.strip()\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation metrics:"
      ],
      "metadata": {
        "id": "B4_hDi4UdLt2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_S1gYjVSqFW"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics\n",
        "def compute_metrics(predictions, references):\n",
        "    bleu_scores = []\n",
        "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores = []\n",
        "    rougeL_scores = []\n",
        "    exact_matches = []\n",
        "    smoothing = SmoothingFunction().method1  # Smoothing to handle zero BLEU scores\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        # BLEU score with smoothing\n",
        "        bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothing)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE score\n",
        "        rouge_scores = rouge_scorer_instance.score(ref, pred)\n",
        "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "        # Exact match\n",
        "        exact_matches.append(1 if pred.strip() == ref.strip() else 0)\n",
        "\n",
        "    return {\n",
        "        \"avg_bleu\": np.mean(bleu_scores),\n",
        "        \"avg_rouge1\": np.mean(rouge1_scores),\n",
        "        \"avg_rougeL\": np.mean(rougeL_scores),\n",
        "        \"exact_match_accuracy\": np.mean(exact_matches)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing predictions:"
      ],
      "metadata": {
        "id": "iRoFcbP8dR6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "btRmPhZmSwW2",
        "outputId": "1ceb9afb-fefe-4d0a-c003-96cad419bca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating predictions for test dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 307/307 [1:40:53<00:00, 19.72s/it]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test dataset\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "print(\"Generating predictions for test dataset...\")\n",
        "for sample in tqdm(test_dataset, desc=\"Evaluating\"):\n",
        "    prompt = prompt_instruction_format(sample)\n",
        "    pred = generate_response(prompt)\n",
        "    predictions.append(pred)\n",
        "    references.append(sample['output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f_nO6XC1S1hw",
        "outputId": "73098466-5058-4388-b2b6-4516a59f861d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Average BLEU Score: 0.0424\n",
            "Average ROUGE-1 F1 Score: 0.2351\n",
            "Average ROUGE-L F1 Score: 0.1889\n",
            "Exact Match Accuracy: 0.0000\n",
            "\n",
            "Sample Predictions vs References (First 5):\n",
            "\n",
            "Sample 1:\n",
            "Input: Hello, sir, your jaw is in pain, especially when you chew or yawn.\n",
            "Prediction: The symptoms seem to indicate that you're suffering from jaw pain. <|END_OF_TEXT|}\n",
            "Reference: In order to confirm the diagnosis, you need to undergo a diagnostic procedure such as an ophthalmology, evaluation, diagnosis, and ophthalmology, as well as a diagnostic procedure for ophthalmology, treatment, nose, mouth, and pharynx, which may require a central line of veins to be inserted to help diagnose and treat the disorder.\n",
            "Tokenized Prediction Length: 19\n",
            "Tokenized Reference Length: 71\n",
            "\n",
            "Sample 2:\n",
            "Input: I will try anything but respectfully how do you account for the desat cycle to 70s for an hr and a half after thr bipap is off?\n",
            "Prediction: I'm not sure what you mean by \"desat cycle\" <|END_OF_TEXT|} <|END_OF_TEXT|}\n",
            "Reference: Could be entirely unrelated to the desaturation when putting the BiPAP on.\n",
            "Tokenized Prediction Length: 25\n",
            "Tokenized Reference Length: 17\n",
            "\n",
            "Sample 3:\n",
            "Input: Sir, I have a problem with impulse control, and sometimes I do things I regret right away, and can you recommend a drug for that?\n",
            "Prediction: Yes, there are several drugs that can help you manage your symptoms. One of the drugs that can help you manage your symptoms is methyldopa. <|END_OF_TEXT|>\n",
            "Reference: Yes, if you're having an impulse control disorder, you can prescribe a drug like Risperidone, Quetiapin, DivalProcess sodium, defacotte, alyphoricol, benztropines, methphenate, bubspa, lysa, escarvel, escarpment, escarlide, or jillin.\n",
            "Tokenized Prediction Length: 37\n",
            "Tokenized Reference Length: 72\n",
            "\n",
            "Sample 4:\n",
            "Input: Doctor, I've recently had a very difficult time speaking, and I've had a very difficult time speaking, and I've also had unusual involuntary movements and sometimes hostile behaviors.\n",
            "Prediction: The symptoms seem to indicate that you're suffering from a neurological disorder that affects your ability to speak, and you have to do some tests to make sure you're diagnosed. <|END_OF_TEXT|>\n",
            "Reference: Your symptoms suggest that you're suffering from delirium, which can cause severe brain damage, causing confusion, disorientation, memory and attention problems.\n",
            "Tokenized Prediction Length: 40\n",
            "Tokenized Reference Length: 30\n",
            "\n",
            "Sample 5:\n",
            "Input: Doctor, I've got all this pain, I've got my voice, and I'm extremely tired, and I've got some pain and energy in my arm, and I need you to tell me what's wrong.\n",
            "Prediction: The symptoms seem to indicate that you're suffering from a muscle spasm. <|END_OF_TEXT|>\n",
            "Reference: The symptoms seem to indicate that the muscle tissue is disassembled, and that the harmful substance is released into the bloodstream, which can cause serious health complications, so you have to be treated by a doctor immediately.\n",
            "Tokenized Prediction Length: 21\n",
            "Tokenized Reference Length: 43\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compute metrics\n",
        "metrics = compute_metrics(predictions, references)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Average BLEU Score: {metrics['avg_bleu']:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1 Score: {metrics['avg_rouge1']:.4f}\")\n",
        "print(f\"Average ROUGE-L F1 Score: {metrics['avg_rougeL']:.4f}\")\n",
        "print(f\"Exact Match Accuracy: {metrics['exact_match_accuracy']:.4f}\")\n",
        "\n",
        "# Print sample predictions for debugging\n",
        "print(\"\\nSample Predictions vs References (First 5):\")\n",
        "for i in range(min(5, len(predictions))):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Input: {test_dataset[i]['input']}\")\n",
        "    print(f\"Prediction: {predictions[i]}\")\n",
        "    print(f\"Reference: {test_dataset[i]['output']}\")\n",
        "    print(f\"Tokenized Prediction Length: {len(tokenizer.encode(predictions[i]))}\")\n",
        "    print(f\"Tokenized Reference Length: {len(tokenizer.encode(test_dataset[i]['output']))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxWl0g3KS3R3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}